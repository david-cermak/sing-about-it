# Learning Sheet Generator

Where LLMs meet melody -- turning knowledge into something you can sing

## ğŸ¯ Overview

An intelligent learning sheet generator that combines web search with AI to create comprehensive educational content. The system searches the web for current information about any topic, intelligently evaluates sources, scrapes full-text content, and uses a local LLM to synthesize it into well-structured learning materials.

## âœ¨ Features

- **ğŸ” Intelligent Web Search**: Multi-query web search using DuckDuckGo for comprehensive topic coverage
- **ğŸ§  Smart Source Evaluation**: AI-powered source evaluation and selection based on relevance and authority
- **ğŸŒ Robust Web Scraping**: Full-text content extraction using multiple methods (newspaper3k, readability, BeautifulSoup)
- **ğŸ“„ Content Processing**: Intelligent cleaning, chunking, and quality validation of scraped content
- **ğŸ¤– AI-Powered Generation**: Local LLM integration via Ollama for content synthesis
- **ğŸ”„ Configurable Agent Backends**: Choose between pydantic.ai framework or direct OpenAI API calls
- **âš™ï¸ Configurable Pipeline**: Environment-based configuration for search parameters, timeouts, and output settings
- **ğŸ“Š Verbose Logging**: Detailed progress tracking showing search queries, results, and processing steps
- **ğŸ—ï¸ Modular Architecture**: Clean separation of concerns with dedicated modules for search, content processing, and generation
- **ğŸ”§ Type Safety**: Pydantic models for robust data validation and structure
- **ğŸ›¡ï¸ Robust Error Handling**: Graceful fallbacks and retry mechanisms for reliable operation
- **ğŸ“‚ Phase Management**: Run individual phases or the complete pipeline with result persistence

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- [Ollama](https://ollama.ai/) running locally with llama3.2 model
- Virtual environment (recommended)

### Installation

1. **Clone and setup:**
   ```bash
   git clone <repository-url>
   cd sing-about-it
   python -m venv venv
   ```

2. **Activate virtual environment:**
   ```powershell
   # Windows PowerShell
   .\venv\Scripts\Activate.ps1

   # Windows Command Prompt
   venv\Scripts\activate.bat

   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment:**
   ```bash
   # Copy and customize the configuration
   cp .env.example .env
   # Edit .env file with your preferred settings
   ```

### Usage

**Run the complete pipeline:**
```bash
python src/main.py --topic "machine learning fundamentals"
```

**Run individual phases:**
```bash
# Phase 1: Search for sources
python src/main.py --phase search --topic "blockchain technology"

# Phase 2: Evaluate and select sources
python src/main.py --phase eval --topic "blockchain technology"

# Phase 3: Scrape content from selected sources
python src/main.py --phase scrape --topic "blockchain technology"

# Phase 4: Generate learning sheet
python src/main.py --phase generate --topic "blockchain technology"
```

**Resume from saved results:**
```bash
# Continue from specific phase using saved results
python src/main.py --phase scrape --file results_evaluation_blockchain_technology.json

# List all saved result files
python src/main.py --list-saves
```

**Run the original single-file version:**
```bash
python src/sheet_generate.py
```

## ğŸ—ï¸ Pipeline Architecture

The system operates through four distinct phases:

### Phase 1: ğŸ” **Web Search**
- Executes multiple targeted search queries per topic
- Uses DuckDuckGo for current, relevant results
- Aggregates and deduplicates sources across queries
- **Output:** `results_search_[topic].json`

### Phase 2: ğŸ§  **Source Evaluation**
- AI-powered evaluation of each source for relevance and authority
- Scores sources based on content type, domain credibility, and topic alignment
- Selects top N sources for scraping (default: 8)
- **Output:** `results_evaluation_[topic].json`

### Phase 3: ğŸŒ **Content Scraping** âœ… **NEW**
- **Multi-method extraction**: newspaper3k â†’ readability â†’ BeautifulSoup fallback
- **Rate limiting**: Respectful delays between requests
- **Content processing**: Cleaning, chunking, and quality validation
- **Quality filtering**: Validates content length, structure, and coherence
- **Output:** `results_scraping_[topic].json`

### Phase 4: ğŸ“ **Learning Sheet Generation**
- Synthesizes collected content into comprehensive learning materials
- Uses full scraped content instead of just snippets (50-100x more information)
- Creates structured, well-sourced educational content
- **Output:** `results_generation_[topic].json`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ” Search     â”‚â”€â”€â”€â–¶â”‚  ğŸ§  Evaluate    â”‚â”€â”€â”€â–¶â”‚  ğŸŒ Scrape      â”‚â”€â”€â”€â–¶â”‚  ğŸ“ Generate    â”‚
â”‚   Multi-query   â”‚    â”‚  AI Selection   â”‚    â”‚  Full Content   â”‚    â”‚  Learning Sheet â”‚
â”‚   Web Search    â”‚    â”‚  Top Sources    â”‚    â”‚  Multi-method   â”‚    â”‚  Synthesis      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       ğŸ“„                      ğŸ“‹                      ğŸ“–                      ğŸ“š
  Search Results         Selected Sources        Content Chunks           Learning Sheet
```

## ğŸ“‹ Configuration

The system uses environment variables for configuration. Key settings include:

- **LLM Configuration**: Model selection, API endpoints, timeouts
- **Agent Backend Selection**: Choose between `pydantic_ai` or `baremetal` backends
- **Search Parameters**: Number of queries, results per query, search depth
- **Source Selection**: Max sources to scrape, minimum relevance score, authority weighting
- **Scraping Configuration**: Timeouts, delays, content limits, extraction methods
- **Content Processing**: Chunking settings, content limits, quality thresholds
- **Logging**: Verbosity levels, debug options, output formatting

### Agent Backend Configuration

Choose your preferred agent backend approach:

```bash
# Use pydantic.ai framework (default) - robust, production-ready
export AGENT_BACKEND=pydantic_ai

# Use baremetal OpenAI API - direct control, simple debugging
export AGENT_BACKEND=baremetal
```

**Pydantic.ai Backend**: Provides structured output validation, automatic retries, and rich debugging capabilities.

**Baremetal Backend**: Direct OpenAI API calls with manual JSON parsing and transparent operation, ideal for debugging and simple deployments.

See `.env.example` for all available configuration options with optimized defaults.

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ main.py                 # Main orchestrator with phase management
â”œâ”€â”€ config/                 # Configuration management
â”‚   â”œâ”€â”€ settings.py        # Environment-based settings
â”‚   â””â”€â”€ models.py          # Pydantic data models
â”œâ”€â”€ search/                 # Web search functionality
â”‚   â”œâ”€â”€ web_search.py      # DuckDuckGo search implementation
â”‚   â””â”€â”€ search_models.py   # Search-related models
â”œâ”€â”€ agents/                 # AI agents with configurable backends
â”‚   â”œâ”€â”€ base.py            # Abstract backend interface
â”‚   â”œâ”€â”€ factory.py         # Backend factory pattern
â”‚   â”œâ”€â”€ backends/          # Backend implementations
â”‚   â”‚   â”œâ”€â”€ pydantic_backend.py  # Pydantic.ai integration
â”‚   â”‚   â””â”€â”€ baremetal_backend.py # Direct OpenAI API calls
â”‚   â”œâ”€â”€ source_evaluator.py     # Original implementation
â”‚   â”œâ”€â”€ source_evaluator_new.py # Configurable backend version
â”‚   â”œâ”€â”€ sheet_generator.py      # Original implementation
â”‚   â””â”€â”€ sheet_generator_new.py  # Configurable backend version
â”œâ”€â”€ scraping/              # Web content extraction âœ… NEW
â”‚   â”œâ”€â”€ web_scraper.py     # Multi-method content scraping
â”‚   â”œâ”€â”€ content_processor.py # Content cleaning and chunking
â”‚   â””â”€â”€ scraper_models.py  # Scraping-related models
â””â”€â”€ utils/                 # Utility functions
    â”œâ”€â”€ text_utils.py      # Text processing
    â””â”€â”€ logging_utils.py   # Enhanced logging
```

### Agent Backend System

The system uses a factory pattern to provide flexible agent backends:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     SourceEvaluatorAgent     â”‚     SheetGeneratorAgent      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  AgentBackend Interface                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      PydanticAIBackend      â”‚      BaremetalBackend         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        pydantic.ai          â”‚      Direct OpenAI API        â”‚
â”‚    - Structured output      â”‚    - Manual JSON parsing      â”‚
â”‚    - Auto retries           â”‚    - Custom retry logic       â”‚
â”‚    - Rich debugging         â”‚    - Raw response access      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Example Output

The enhanced system generates comprehensive learning sheets with:
- **Rich Content**: Full-text information from 50-100x more source material
- **Structured Format**: Clear headings, bullet points, and examples
- **Current Information**: Up-to-date web search results incorporated
- **Source Attribution**: Links to all referenced sources
- **Comprehensive Coverage**: Multiple perspectives and detailed explanations
- **Quality Assurance**: Content validation and intelligent filtering

### Content Volume Improvement
- **Before**: ~100 characters per source (snippets only)
- **After**: ~5,000+ characters per source (full article content)
- **Result**: 50-100x more information for LLM synthesis

## ğŸ§ª Testing

**Test the web scraping functionality:**
```bash
python test_web_scraping.py
```

**Test the agent backend system:**
```bash
python test_agent_backends.py
```

These tests will:
- Demonstrate scraping with multiple extraction methods
- Show content processing and quality validation
- Test both pydantic.ai and baremetal backends
- Compare performance and reliability
- Show detailed output for debugging

For detailed usage instructions and examples, see [BACKEND_USAGE.md](BACKEND_USAGE.md).

## ğŸ”§ Dependencies

### Core Dependencies
- **pydantic-ai**: AI agent framework (optional - for pydantic.ai backend)
- **openai**: OpenAI API client (for both backend types)
- **ollama**: Local LLM integration
- **ddgs**: DuckDuckGo search functionality
- **python-dotenv**: Environment configuration

### Web Scraping Dependencies âœ… NEW
- **newspaper3k**: Primary article extraction
- **readability-lxml**: Content extraction fallback
- **beautifulsoup4**: HTML parsing and cleaning
- **requests**: HTTP client for scraping
- **fake-useragent**: Rotating user agents for respectful scraping
- **markdownify**: HTML to Markdown conversion
- **validators**: URL validation

### Additional Libraries
- **aiohttp**: Async HTTP capabilities (future enhancement)
- **python-magic**: File type detection
- **urllib3**: Advanced URL handling

## ğŸ¯ Implementation Status

- âœ… **Phase 1**: Modular architecture with configuration management
- âœ… **Phase 2**: Intelligent source evaluation with LLM agents
- âœ… **Phase 3**: Robust web scraping with multiple extraction methods
- â³ **Phase 4**: Enhanced generation with full content integration

**Next:** Integrate scraped content into learning sheet generation for dramatically improved quality and comprehensiveness.
